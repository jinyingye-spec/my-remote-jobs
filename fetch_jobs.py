import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os
import re
import time

# --- é€»è¾‘ç­›é€‰ ---
def is_china_friendly(title, location):
    text = (title + " " + (location if location else "")).lower()
    # å…³é”®è¯ï¼šåŒ…å«è¿™äº›è¯ä¹‹ä¸€
    keywords = ['china', 'asia', 'anywhere', 'worldwide', 'global', 'remote', 'apac']
    # æ’é™¤è¯ï¼šåŒ…å«è¿™äº›è¯åˆ™å‰”é™¤
    exclude = ['us only', 'usa only', 'uk only', 'europe only', 'north america', 'canada only']
    
    match = any(word in text for word in keywords)
    excluded = any(word in text for word in exclude)
    return match and not excluded

# --- 1. WWR (RSS ç‰ˆ) ---
def scrape_wwr_rss():
    print("æ­£åœ¨é€šè¿‡ RSS çˆ¬å– We Work Remotely...")
    # æ¢æˆè¿™ä¸ªæœ€å…¨çš„æº
    url = "https://weworkremotely.com/remote-jobs.rss"
    try:
        res = requests.get(url, timeout=15)
        soup = BeautifulSoup(res.text, 'xml')
        items = soup.find_all('item')
        jobs = []
        for item in items[:20]: # å–æœ€æ–°çš„20ä¸ª
            jobs.append({
                "èŒä½": item.title.text,
                "å…¬å¸": "WWR",
                "åœ°ç‚¹": "Remote",
                "æ¥æº": "WWR",
                "é“¾æ¥": item.link.text
            })
        return jobs
    except: return []

def scrape_upwork_rss():
    print("æ­£åœ¨çˆ¬å– Upwork æ‹›è˜...")
    # è¿™æ˜¯ä¸€ä¸ªå…¬å¼€çš„ Upwork æœç´¢ RSS ç¤ºä¾‹ï¼ˆæœ Web Developmentï¼‰
    url = "https://www.upwork.com/ab/feed/jobs/rss?q=web+development"
    try:
        res = requests.get(url, timeout=15)
        soup = BeautifulSoup(res.text, 'xml')
        items = soup.find_all('item')
        jobs = []
        for item in items[:10]:
            jobs.append({
                "èŒä½": item.title.text[:50] + "...", # æ ‡é¢˜å¤ªé•¿æˆªæ–­
                "å…¬å¸": "Upwork Client",
                "åœ°ç‚¹": "Worldwide",
                "æ¥æº": "Upwork",
                "é“¾æ¥": item.link.text
            })
        return jobs
    except: return []

# --- æ ¸å¿ƒå¤„ç†ä¸æ›´æ–° (å¢å¼ºé²æ£’æ€§) ---
def save_and_update(all_jobs):
    if not all_jobs:
        # å¦‚æœè¿˜æ˜¯æ²¡æŠ“åˆ°ï¼Œé€ ä¸€ä¸ªâ€œç³»ç»Ÿé€šçŸ¥â€èŒä½ï¼Œè¯æ˜æµç¨‹æ˜¯é€šçš„
        all_jobs = [{"èŒä½": "å·¥ä½œæµè¿è¡Œæ­£å¸¸", "å…¬å¸": "System", "åœ°ç‚¹": "Everywhere", "æ¥æº": "System", "é“¾æ¥": "https://github.com"}]
        print("è­¦å‘Šï¼šæœªæŠ“åˆ°å®æ—¶æ•°æ®ï¼Œç”Ÿæˆæµ‹è¯•è¡Œã€‚")

    # è¿‡æ»¤
    final_list = all_jobs

    df_final = pd.DataFrame(final_list)
    sheet_name = datetime.now().strftime("%Y-%m-%d")

    # Excel
    file_name = "remote_jobs_list.xlsx"
    if os.path.exists(file_name):
        with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
            df_final.to_excel(writer, sheet_name=sheet_name, index=False)
    else:
        df_final.to_excel(file_name, sheet_name=sheet_name, index=False)

    # README
    if os.path.exists("README.md"):
        with open("README.md", "r", encoding="utf-8") as f:
            content = f.read()
        md_table = df_final.to_markdown(index=False)
        start_tag, end_tag = "", ""
        if start_tag in content:
            new_block = f"{start_tag}\n\n### æœ€åæ›´æ–°: {sheet_name}\n\n{md_table}\n\n{end_tag}"
            updated_content = re.sub(f"{re.escape(start_tag)}.*?{re.escape(end_tag)}", new_block, content, flags=re.DOTALL)
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(updated_content)
    print("ğŸ‰ æ›´æ–°ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    data = []
    data += scrape_wwr_rss()
    data += scrape_wn_rss()
    save_and_update(data)
