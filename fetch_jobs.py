import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os, re, time

HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

def scrape_remote_ok():
    print("æ­£åœ¨æŠ“å– Remote OK...")
    try:
        res = requests.get("https://remoteok.com/api", headers=HEADERS, timeout=15)
        data = res.json()
        jobs = []
        # å¼ºåˆ¶åˆ‡ç‰‡ data[1:11]ï¼Œç¡®ä¿åªå–å‰10æ¡
        for item in data[1:11]: 
            jobs.append({
                "èŒä½": item.get('position', 'N/A'),
                "å…¬å¸": item.get('company', 'N/A'),
                "æ¥æº": "RemoteOK",
                "é“¾æ¥": item.get('url', '')
            })
        return jobs
    except: return []

def scrape_wwr():
    print("æ­£åœ¨æŠ“å– We Work Remotely...")
    try:
        res = requests.get("https://weworkremotely.com/remote-jobs.rss", timeout=15)
        soup = BeautifulSoup(res.text, 'xml')
        items = soup.find_all('item')
        jobs = []
        for item in items[:10]: # é™åˆ¶ 10 æ¡
            jobs.append({
                "èŒä½": item.title.text.strip(),
                "å…¬å¸": "WWR",
                "æ¥æº": "WWR",
                "é“¾æ¥": item.link.text.strip()
            })
        return jobs
    except: return []

def scrape_working_nomads():
    print("æ­£åœ¨æŠ“å– Working Nomads...")
    try:
        res = requests.get("https://www.workingnomads.com/jobsapi/rss/jobs?category=development", timeout=15)
        soup = BeautifulSoup(res.text, 'xml')
        items = soup.find_all('item')
        jobs = []
        for item in items[:10]: # é™åˆ¶ 10 æ¡
            jobs.append({
                "èŒä½": item.title.text.strip(),
                "å…¬å¸": "WorkingNomads",
                "æ¥æº": "WN",
                "é“¾æ¥": item.link.text.strip()
            })
        return jobs
    except: return []

def save_and_update(all_jobs):
    if not all_jobs: return
    
    # è½¬æ¢ä¸º DataFrame
    df = pd.DataFrame(all_jobs)
    # å¼ºåˆ¶åªæ˜¾ç¤ºä»¥ä¸‹å››åˆ—ï¼Œé˜²æ­¢è¡¨æ ¼è¿‡å®½
    df = df[["èŒä½", "å…¬å¸", "æ¥æº", "é“¾æ¥"]] 
    
    today_str = datetime.now().strftime("%Y-%m-%d")

    # A. ä¿å­˜ Excel
    file_name = "remote_jobs_list.xlsx"
    if os.path.exists(file_name):
        with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
            df.to_excel(writer, sheet_name=today_str, index=False)
    else:
        df.to_excel(file_name, sheet_name=today_str, index=False)

    # B. æ›´æ–° README (æ ¸å¿ƒæ­£åˆ™æ›¿æ¢)
    if os.path.exists("README.md"):
        with open("README.md", "r", encoding="utf-8") as f:
            content = f.read()
        
        md_table = df.to_markdown(index=False)
        start_tag, end_tag = "", ""
        
        if start_tag in content and end_tag in content:
            # ä½¿ç”¨ re.DOTALL ç¡®ä¿è·¨è¡ŒåŒ¹é…ï¼ŒæŠŠæ—§çš„ 300 å¤šè¡Œä¸€æ¬¡æ€§å¹²æ‰
            pattern = re.compile(f"{re.escape(start_tag)}.*?{re.escape(end_tag)}", re.DOTALL)
            new_block = f"{start_tag}\n\n### ğŸ“… æœ¬æ¬¡ç²¾é€‰èŒä½ ({today_str})\n\n{md_table}\n\n{end_tag}"
            updated_content = pattern.sub(new_block, content)
            
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(updated_content)
        print(f"âœ… æ›´æ–°å®Œæˆï¼Œæ€»è®¡å±•ç¤º {len(all_jobs)} æ¡å„æ¥æºç²¾é€‰èŒä½")

if __name__ == "__main__":
    combined = []
    # å»ºè®®æŠŠ RemoteOK æ”¾åœ¨æœ€åï¼Œè®© WWR å’Œ WN ä¼˜å…ˆå±•ç¤º
    combined += scrape_wwr()
    combined += scrape_working_nomads()
    combined += scrape_remote_ok()
    
    save_and_update(combined)
