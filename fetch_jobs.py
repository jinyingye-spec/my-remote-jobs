import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os, re, time

HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

# --- æŠ“å–å‡½æ•°ï¼šæ¯ä¸ªéƒ½å¼ºåˆ¶åªè¿”å›å‰ 10 ä¸ª ---

def scrape_remote_ok():
    print("æ­£åœ¨æŠ“å– Remote OK...")
    try:
        res = requests.get("https://remoteok.com/api", headers=HEADERS, timeout=15)
        data = res.json()
        jobs = []
        for item in data[1:11]: # åªå– API è¿”å›çš„å‰ 10 ä¸ªæœ‰æ•ˆèŒä½
            jobs.append({
                "èŒä½": item.get('position', 'N/A'),
                "å…¬å¸": item.get('company', 'N/A'),
                "æ¥æº": "RemoteOK",
                "é“¾æ¥": item.get('url', '')
            })
        return jobs
    except: return []

def scrape_wwr():
    print("æ­£åœ¨æŠ“å– We Work Remotely...")
    try:
        res = requests.get("https://weworkremotely.com/remote-jobs.rss", timeout=15)
        soup = BeautifulSoup(res.text, 'xml')
        items = soup.find_all('item')
        jobs = []
        for item in items[:10]: # å¼ºåˆ¶é™åˆ¶ 10 æ¡
            jobs.append({
                "èŒä½": item.title.text.strip(),
                "å…¬å¸": "WWR",
                "æ¥æº": "WWR",
                "é“¾æ¥": item.link.text.strip()
            })
        return jobs
    except: return []

def scrape_working_nomads():
    print("æ­£åœ¨æŠ“å– Working Nomads...")
    try:
        res = requests.get("https://www.workingnomads.com/jobsapi/rss/jobs?category=development", timeout=15)
        soup = BeautifulSoup(res.text, 'xml')
        items = soup.find_all('item')
        jobs = []
        for item in items[:10]: # å¼ºåˆ¶é™åˆ¶ 10 æ¡
            jobs.append({
                "èŒä½": item.title.text.strip(),
                "å…¬å¸": "WorkingNomads",
                "æ¥æº": "WN",
                "é“¾æ¥": item.link.text.strip()
            })
        return jobs
    except: return []

# --- ç»Ÿä¸€æ›´æ–°é€»è¾‘ ---

def save_and_update(all_jobs):
    if not all_jobs: return
    
    # è½¬æ¢ä¸º DataFrame å¹¶æ•´ç†
    df = pd.DataFrame(all_jobs)
    # åªä¿ç•™è¿™å‡ åˆ—ï¼Œè®©è¡¨æ ¼åœ¨æ‰‹æœºç«¯çœ‹ä¹Ÿä¸æŒ¤
    df = df[["èŒä½", "å…¬å¸", "æ¥æº", "é“¾æ¥"]] 
    
    today_str = datetime.now().strftime("%Y-%m-%d")

    # A. ä¿å­˜ Excel
    file_name = "remote_jobs_list.xlsx"
    if os.path.exists(file_name):
        with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
            df.to_excel(writer, sheet_name=today_str, index=False)
    else:
        df.to_excel(file_name, sheet_name=today_str, index=False)

    # B. æ›´æ–° README (ä½¿ç”¨æ­£åˆ™å½»åº•æ›¿æ¢)
    if os.path.exists("README.md"):
        with open("README.md", "r", encoding="utf-8") as f:
            content = f.read()
        
        md_table = df.to_markdown(index=False)
        start_tag, end_tag = "", ""
        
        if start_tag in content and end_tag in content:
            pattern = re.compile(f"{re.escape(start_tag)}.*?{re.escape(end_tag)}", re.DOTALL)
            new_block = f"{start_tag}\n\n### ğŸ“… æœ¬æ¬¡èšåˆæœ€æ–°èŒä½ ({today_str})\n\n{md_table}\n\n{end_tag}"
            updated_content = pattern.sub(new_block, content)
            
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(updated_content)
        print(f"âœ… æ±‡æ€»å®Œæˆï¼Œæ€»è®¡å±•ç¤º {len(all_jobs)} æ¡ç²¾é€‰èŒä½")

if __name__ == "__main__":
    combined = []
    # ä¾æ¬¡æ·»åŠ ï¼Œå¦‚æœæŸä¸ªç«™æŒ‚äº†ï¼Œä¹Ÿä¸å½±å“åˆ«çš„ç«™
    combined += scrape_wwr()
    combined += scrape_working_nomads()
    combined += scrape_remote_ok()
    
    save_and_update(combined)
