import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os, re, time

# --- 1. å¢å¼ºç‰ˆè¯·æ±‚é…ç½® ---
HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

# --- 2. ç¨³å¥çš„æŠ“å–å‡½æ•° ---

def scrape_remote_ok():
    print("æ­£åœ¨çˆ¬å– Remote OK (APIæ¨¡å¼)...")
    url = "https://remoteok.com/api" # ä½¿ç”¨å…¶å…¬å¼€APIï¼Œæ¯”ç½‘é¡µæ›´ç¨³
    try:
        res = requests.get(url, headers=HEADERS, timeout=15)
        data = res.json()
        jobs = []
        # APIç¬¬ä¸€ä¸ªå…ƒç´ é€šå¸¸æ˜¯å£°æ˜ï¼Œè·³è¿‡
        for item in data[1:15]:
            jobs.append({
                "èŒä½": item.get('position', 'N/A'),
                "å…¬å¸": item.get('company', 'N/A'),
                "åœ°ç‚¹": "Worldwide",
                "æ¥æº": "RemoteOK",
                "é“¾æ¥": item.get('url', '')
            })
        print(f"Remote OK æŠ“å–æˆåŠŸ: {len(jobs)} æ¡")
        return jobs
    except: return []

def scrape_wwr_rss():
    print("æ­£åœ¨çˆ¬å– We Work Remotely (RSS)...")
    url = "https://weworkremotely.com/remote-jobs.rss"
    try:
        res = requests.get(url, timeout=15)
        soup = BeautifulSoup(res.text, 'xml')
        items = soup.find_all('item')
        jobs = []
        for item in items[:15]:
            jobs.append({
                "èŒä½": item.title.text.strip(),
                "å…¬å¸": "WWR",
                "åœ°ç‚¹": "Remote",
                "æ¥æº": "WWR",
                "é“¾æ¥": item.link.text.strip()
            })
        print(f"WWR æŠ“å–æˆåŠŸ: {len(jobs)} æ¡")
        return jobs
    except: return []

# --- 3. æ ¸å¿ƒä¿å­˜ä¸æ›¿æ¢é€»è¾‘ ---

def save_and_update(all_jobs):
    if not all_jobs:
        all_jobs = [{"èŒä½": "æ­£åœ¨ç­‰å¾…æ–°èŒä½å‘å¸ƒ...", "å…¬å¸": "-", "åœ°ç‚¹": "-", "æ¥æº": "System", "é“¾æ¥": "#"}]

    df = pd.DataFrame(all_jobs)
    today_str = datetime.now().strftime("%Y-%m-%d")

    # A. æ›´æ–° Excel (ä¿æŒä¸å˜)
    file_name = "remote_jobs_list.xlsx"
    if os.path.exists(file_name):
        with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
            df.to_excel(writer, sheet_name=today_str, index=False)
    else:
        df.to_excel(file_name, sheet_name=today_str, index=False)

    # B. æ›´æ–° README (å½»åº•è§£å†³é‡å¤é—®é¢˜)
    if os.path.exists("README.md"):
        with open("README.md", "r", encoding="utf-8") as f:
            content = f.read()
        
        md_table = df.to_markdown(index=False)
        start_tag, end_tag = "", ""
        
        if start_tag in content and end_tag in content:
            # è¿™é‡Œçš„æ­£åˆ™ä¼šåƒæ‰ä¸¤ä¸ªæ ‡ç­¾ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ï¼ŒåŒ…æ‹¬æ—§çš„æ—¥æœŸå’Œè¡¨æ ¼
            pattern = re.compile(f"{re.escape(start_tag)}.*?{re.escape(end_tag)}", re.DOTALL)
            new_block = f"{start_tag}\n\n### ğŸ“… æœ€åæ›´æ–°: {today_str}\n\n{md_table}\n\n{end_tag}"
            updated_content = pattern.sub(new_block, content)
            
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(updated_content)
            print("âœ… README.md æ›´æ–°æˆåŠŸï¼")

if __name__ == "__main__":
    data = []
    data += scrape_remote_ok()
    time.sleep(2)
    data += scrape_wwr_rss()
    save_and_update(data)
