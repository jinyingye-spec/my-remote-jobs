import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os
import re
import time

# --- é€»è¾‘ç­›é€‰ ---
def is_china_friendly(title, location):
    text = (title + " " + (location if location else "")).lower()
    # å…³é”®è¯ï¼šåŒ…å«è¿™äº›è¯ä¹‹ä¸€
    keywords = ['china', 'asia', 'anywhere', 'worldwide', 'global', 'remote', 'apac']
    # æ’é™¤è¯ï¼šåŒ…å«è¿™äº›è¯åˆ™å‰”é™¤
    exclude = ['us only', 'usa only', 'uk only', 'europe only', 'north america', 'canada only']
    
    match = any(word in text for word in keywords)
    excluded = any(word in text for word in exclude)
    return match and not excluded

# --- 1. WWR (RSS ç‰ˆ) ---
def scrape_wwr_rss():
    print("æ­£åœ¨é€šè¿‡ RSS çˆ¬å– We Work Remotely...")
    url = "https://weworkremotely.com/categories/remote-software-development-jobs.rss"
    try:
        res = requests.get(url, timeout=15)
        soup = BeautifulSoup(res.text, 'xml') # ä½¿ç”¨ XML è§£æå™¨
        items = soup.find_all('item')
        jobs = []
        for item in items:
            title = item.title.text
            link = item.link.text
            # RSS é€šå¸¸åœ¨æè¿°é‡ŒåŒ…å«å…¬å¸å
            company = item.find('dc:creator').text if item.find('dc:creator') else "Remote Co"
            jobs.append({"èŒä½": title, "å…¬å¸": company, "åœ°ç‚¹": "Global/Remote", "æ¥æº": "WWR", "é“¾æ¥": link})
        print(f"WWR RSS æŠ“å–æˆåŠŸ: {len(jobs)} æ¡")
        return jobs
    except Exception as e:
        print(f"WWR RSS å‡ºé”™: {e}"); return []

# --- 2. Working Nomads (RSS ç‰ˆ) ---
def scrape_wn_rss():
    print("æ­£åœ¨é€šè¿‡ RSS çˆ¬å– Working Nomads...")
    url = "https://www.workingnomads.com/jobsapi/rss/jobs?category=development"
    try:
        res = requests.get(url, timeout=15)
        soup = BeautifulSoup(res.text, 'xml')
        items = soup.find_all('item')
        jobs = []
        for item in items:
            jobs.append({
                "èŒä½": item.title.text,
                "å…¬å¸": "Working Nomads",
                "åœ°ç‚¹": "Remote",
                "æ¥æº": "Working Nomads",
                "é“¾æ¥": item.link.text
            })
        print(f"Working Nomads RSS æŠ“å–æˆåŠŸ: {len(jobs)} æ¡")
        return jobs
    except Exception as e:
        print(f"WN RSS å‡ºé”™: {e}"); return []

# --- æ ¸å¿ƒå¤„ç†ä¸æ›´æ–° (å¢å¼ºé²æ£’æ€§) ---
def save_and_update(all_jobs):
    if not all_jobs:
        # å¦‚æœè¿˜æ˜¯æ²¡æŠ“åˆ°ï¼Œé€ ä¸€ä¸ªâ€œç³»ç»Ÿé€šçŸ¥â€èŒä½ï¼Œè¯æ˜æµç¨‹æ˜¯é€šçš„
        all_jobs = [{"èŒä½": "å·¥ä½œæµè¿è¡Œæ­£å¸¸", "å…¬å¸": "System", "åœ°ç‚¹": "Everywhere", "æ¥æº": "System", "é“¾æ¥": "https://github.com"}]
        print("è­¦å‘Šï¼šæœªæŠ“åˆ°å®æ—¶æ•°æ®ï¼Œç”Ÿæˆæµ‹è¯•è¡Œã€‚")

    # è¿‡æ»¤
    final_list = [j for j in all_jobs if is_china_friendly(j['èŒä½'], j['åœ°ç‚¹'])]
    if not final_list: final_list = all_jobs[:10] # å¦‚æœè¿‡æ»¤å®Œæ²¡äº†ï¼Œå°±å–å‰10ä¸ªä¿åº•

    df_final = pd.DataFrame(final_list)
    sheet_name = datetime.now().strftime("%Y-%m-%d")

    # Excel
    file_name = "remote_jobs_list.xlsx"
    if os.path.exists(file_name):
        with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
            df_final.to_excel(writer, sheet_name=sheet_name, index=False)
    else:
        df_final.to_excel(file_name, sheet_name=sheet_name, index=False)

    # README
    if os.path.exists("README.md"):
        with open("README.md", "r", encoding="utf-8") as f:
            content = f.read()
        md_table = df_final.to_markdown(index=False)
        start_tag, end_tag = "", ""
        if start_tag in content:
            new_block = f"{start_tag}\n\n### æœ€åæ›´æ–°: {sheet_name}\n\n{md_table}\n\n{end_tag}"
            updated_content = re.sub(f"{re.escape(start_tag)}.*?{re.escape(end_tag)}", new_block, content, flags=re.DOTALL)
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(updated_content)
    print("ğŸ‰ æ›´æ–°ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    data = []
    data += scrape_wwr_rss()
    data += scrape_wn_rss()
    save_and_update(data)
