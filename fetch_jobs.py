import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os, re, time

# --- 1. é…ç½® ---
HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

# --- 2. ç¨³å®šçš„æŠ“å–å‡½æ•°åº“ ---

def scrape_remote_ok():
    print("æ­£åœ¨æŠ“å– Remote OK...")
    url = "https://remoteok.com/api"
    try:
        res = requests.get(url, headers=HEADERS, timeout=15)
        data = res.json()
        jobs = []
        for item in data[1:11]: # ä¸¥æ ¼é™åˆ¶ 10 æ¡
            jobs.append({
                "èŒä½": item.get('position', 'N/A'),
                "å…¬å¸": item.get('company', 'N/A'),
                "åœ°ç‚¹": "Worldwide",
                "æ¥æº": "RemoteOK",
                "é“¾æ¥": item.get('url', '')
            })
        return jobs
    except: return []

def scrape_wwr():
    print("æ­£åœ¨æŠ“å– We Work Remotely...")
    url = "https://weworkremotely.com/remote-jobs.rss"
    try:
        res = requests.get(url, timeout=15)
        soup = BeautifulSoup(res.text, 'xml')
        items = soup.find_all('item')
        jobs = []
        for item in items[:10]: # ä¸¥æ ¼é™åˆ¶ 10 æ¡
            jobs.append({
                "èŒä½": item.title.text.strip(),
                "å…¬å¸": "WWR",
                "åœ°ç‚¹": "Remote",
                "æ¥æº": "WWR",
                "é“¾æ¥": item.link.text.strip()
            })
        return jobs
    except: return []

def scrape_working_nomads():
    print("æ­£åœ¨æŠ“å– Working Nomads...")
    url = "https://www.workingnomads.com/jobsapi/rss/jobs?category=development"
    try:
        res = requests.get(url, timeout=15)
        soup = BeautifulSoup(res.text, 'xml')
        items = soup.find_all('item')
        jobs = []
        for item in items[:10]: # ä¸¥æ ¼é™åˆ¶ 10 æ¡
            jobs.append({
                "èŒä½": item.title.text.strip(),
                "å…¬å¸": "WorkingNomads",
                "åœ°ç‚¹": "Global",
                "æ¥æº": "WN",
                "é“¾æ¥": item.link.text.strip()
            })
        return jobs
    except: return []

def scrape_just_remote():
    print("æ­£åœ¨æŠ“å– JustRemote...")
    # JustRemote æ²¡æœ‰å…¬å¼€ RSSï¼Œæˆ‘ä»¬å°è¯•é€šè¿‡å…¶èŒä½çš„åˆ—è¡¨é¡µè§£æ
    url = "https://justremote.co/remote-developer-jobs"
    try:
        res = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select('.job-item')
        jobs = []
        for item in items[:10]: # ä¸¥æ ¼é™åˆ¶ 10 æ¡
            title = item.find('h3').text.strip() if item.find('h3') else "N/A"
            company = item.find('div', class_='company').text.strip() if item.find('div', class_='company') else "N/A"
            link = "https://justremote.co" + item.find('a')['href']
            jobs.append({"èŒä½": title, "å…¬å¸": company, "åœ°ç‚¹": "Remote", "æ¥æº": "JustRemote", "é“¾æ¥": link})
        return jobs
    except: return []

# --- 3. ä¿å­˜ä¸æ›´æ–° ---

def save_and_update(all_jobs):
    if not all_jobs: return
    
    df = pd.DataFrame(all_jobs)
    today_str = datetime.now().strftime("%Y-%m-%d")

    # æ›´æ–° Excel
    file_name = "remote_jobs_list.xlsx"
    if os.path.exists(file_name):
        with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
            df.to_excel(writer, sheet_name=today_str, index=False)
    else:
        df.to_excel(file_name, sheet_name=today_str, index=False)

    # æ›´æ–° README
    if os.path.exists("README.md"):
        with open("README.md", "r", encoding="utf-8") as f:
            content = f.read()
        
        md_table = df.to_markdown(index=False)
        start_tag, end_tag = "", ""
        
        if start_tag in content and end_tag in content:
            # è¿™é‡Œçš„æ­£åˆ™é€»è¾‘ä¼šæŠŠä¸¤ä¸ªæ ‡ç­¾ä¸­é—´çš„æ‰€æœ‰æ—§å†…å®¹ä¸€æ¬¡æ€§æ¸…é™¤å¹¶å¡«å…¥æ–°çš„
            pattern = re.compile(f"{re.escape(start_tag)}.*?{re.escape(end_tag)}", re.DOTALL)
            new_block = f"{start_tag}\n\n### ğŸ“… æœ€åæ›´æ–°æ—¶é—´: {today_str}\n\n{md_table}\n\n{end_tag}"
            updated_content = pattern.sub(new_block, content)
            
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(updated_content)
        print(f"ğŸ‰ æˆåŠŸæ•´åˆæŠ“å–äº† {len(all_jobs)} æ¡èŒä½ï¼")

if __name__ == "__main__":
    combined = []
    combined += scrape_remote_ok()
    time.sleep(1)
    combined += scrape_wwr()
    time.sleep(1)
    combined += scrape_working_nomads()
    time.sleep(1)
    combined += scrape_just_remote()
    
    save_and_update(combined)
