import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os, re, time

HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

def scrape_remote_ok():
    print(">>> æ­£åœ¨å¯åŠ¨ Remote OK æŠ“å–...")
    try:
        res = requests.get("https://remoteok.com/api", headers=HEADERS, timeout=15)
        data = res.json()
        # å¼ºåˆ¶ï¼šåªå– API è¿”å›çš„å‰ 10 ä¸ªï¼ˆè·³è¿‡ç¬¬ä¸€ä¸ªæ³•å¾‹å£°æ˜ï¼‰
        subset = data[1:11] 
        jobs = [{"èŒä½": j.get('position'), "å…¬å¸": j.get('company'), "æ¥æº": "RemoteOK", "é“¾æ¥": j.get('url')} for j in subset]
        print(f"DEBUG: RemoteOK æˆåŠŸè·å– {len(jobs)} æ¡")
        return jobs
    except Exception as e:
        print(f"DEBUG: RemoteOK æŠ¥é”™ - {e}")
        return []

def scrape_wwr():
    print(">>> æ­£åœ¨å¯åŠ¨ WWR æŠ“å–...")
    try:
        res = requests.get("https://weworkremotely.com/remote-jobs.rss", timeout=15)
        # å°† 'xml' æ”¹ä¸º 'xml' æˆ– 'html.parser'ï¼Œä½†å‰ææ˜¯å®‰è£…äº† lxml
        soup = BeautifulSoup(res.text, 'xml') 
        items = soup.find_all('item')[:10]
        jobs = [{"èŒä½": i.title.text, "å…¬å¸": "WWR", "æ¥æº": "WWR", "é“¾æ¥": i.link.text} for i in items]
        print(f"DEBUG: WWR æˆåŠŸè·å– {len(jobs)} æ¡")
        return jobs
    except Exception as e:
        print(f"DEBUG: WWR æŠ¥é”™ - {e}")
        return []

def scrape_working_nomads():
    print(">>> æ­£åœ¨å¯åŠ¨ Working Nomads æŠ“å–...")
    try:
        res = requests.get("https://www.workingnomads.com/jobsapi/rss/jobs?category=development", timeout=15)
        soup = BeautifulSoup(res.text, 'xml') # åŒæ ·ç¡®ä¿è¿™é‡Œèƒ½ç”¨ xml
        items = soup.find_all('item')[:10]
        jobs = [{"èŒä½": i.title.text, "å…¬å¸": "WorkingNomads", "æ¥æº": "WN", "é“¾æ¥": i.link.text} for i in items]
        print(f"DEBUG: WN æˆåŠŸè·å– {len(jobs)} æ¡")
        return jobs
    except Exception as e:
        print(f"DEBUG: WN æŠ¥é”™ - {e}")
        return []

def save_and_update(all_jobs):
    if not all_jobs:
        print("CRITICAL: æ‰€æœ‰æºå‡ä¸ºç©ºï¼Œä¸æ›´æ–°æ–‡ä»¶ã€‚")
        return

    # æœ€ç»ˆé˜²å¾¡ï¼šä¸ç®¡å‰é¢å‘ç”Ÿäº†ä»€ä¹ˆï¼Œæ€»è¡¨åªç•™å‰ 30 è¡Œ
    final_jobs = all_jobs[:30]
    df = pd.DataFrame(final_jobs)[["èŒä½", "å…¬å¸", "æ¥æº", "é“¾æ¥"]]
    today = datetime.now().strftime("%Y-%m-%d")

    # A. æ›´æ–° Excel
    try:
        file_name = "remote_jobs_list.xlsx"
        if os.path.exists(file_name):
            with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
                df.to_excel(writer, sheet_name=today, index=False)
        else:
            df.to_excel(file_name, sheet_name=today, index=False)
    except: pass

    # B. æ›´æ–° README
    if os.path.exists("README.md"):
        with open("README.md", "r", encoding="utf-8") as f:
            content = f.read()
        
        start_tag, end_tag = "", ""
        if start_tag in content and end_tag in content:
            md_table = df.to_markdown(index=False)
            new_block = f"{start_tag}\n\n### ğŸ“… æ›´æ–°: {today}\n\n{md_table}\n\n{end_tag}"
            # ä½¿ç”¨ re.DOTALL ç¡®ä¿èƒ½æ›¿æ¢ä¸­é—´å¤šè¡Œæ•°æ®
            updated = re.sub(f"{re.escape(start_tag)}.*?{re.escape(end_tag)}", new_block, content, flags=re.DOTALL)
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(updated)
            print(f"SUCCESS: å·²å†™å…¥ {len(final_jobs)} æ¡èŒä½åˆ° README")

if __name__ == "__main__":
    combined = []
    # è°ƒæ•´é¡ºåºï¼šæŠŠæœ€ç¨³çš„ RSS æ”¾å‰é¢
    combined += scrape_wwr()
    combined += scrape_working_nomads()
    combined += scrape_remote_ok()
    
    save_and_update(combined)
