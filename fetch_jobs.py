import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os
import re
import time

# --- 1. åŸºç¡€é…ç½® ---
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# --- 2. çˆ¬å–å‡½æ•° (ç»Ÿä¸€é‡‡ç”¨ RSS æ¨¡å¼ï¼Œæœ€ç¨³å®š) ---

def scrape_wwr_rss():
    print("æ­£åœ¨é€šè¿‡ RSS çˆ¬å– We Work Remotely...")
    url = "https://weworkremotely.com/remote-jobs.rss"
    jobs = []
    try:
        res = requests.get(url, timeout=15)
        # ä½¿ç”¨ç®€å•çš„æ­£åˆ™æˆ–å†…ç½®è§£æï¼Œå‡å°‘å¯¹ lxml çš„ä¾èµ–æŠ¥é”™
        soup = BeautifulSoup(res.text, 'xml')
        items = soup.find_all('item')
        for item in items[:15]: # å–å‰15æ¡
            jobs.append({
                "èŒä½": item.title.text.strip(),
                "å…¬å¸": "WWR",
                "åœ°ç‚¹": "Remote",
                "æ¥æº": "WWR",
                "é“¾æ¥": item.link.text.strip()
            })
    except Exception as e:
        print(f"WWR RSS æŠ“å–å¤±è´¥: {e}")
    return jobs

def scrape_upwork_rss():
    print("æ­£åœ¨é€šè¿‡ RSS çˆ¬å– Upwork...")
    # æœç´¢ 'python' ç›¸å…³çš„è¿œç¨‹èŒä½
    url = "https://www.upwork.com/ab/feed/jobs/rss?q=python&sort=recency"
    jobs = []
    try:
        res = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'xml')
        items = soup.find_all('item')
        for item in items[:10]:
            jobs.append({
                "èŒä½": item.title.text.strip()[:60] + "...",
                "å…¬å¸": "Upwork Client",
                "åœ°ç‚¹": "Worldwide",
                "æ¥æº": "Upwork",
                "é“¾æ¥": item.link.text.strip()
            })
    except Exception as e:
        print(f"Upwork RSS æŠ“å–å¤±è´¥: {e}")
    return jobs

# --- 3. æ ¸å¿ƒä¿å­˜é€»è¾‘ ---

def save_and_update(all_jobs):
    if not all_jobs:
        # å¦‚æœå•¥ä¹Ÿæ²¡æŠ“åˆ°ï¼Œç”Ÿæˆä¸€æ¡ä¿åº•æ•°æ®ï¼Œé˜²æ­¢ Action æŠ¥é”™
        all_jobs = [{"èŒä½": "æš‚æ— æ–°èŒä½ (æ£€æŸ¥æº)", "å…¬å¸": "-", "åœ°ç‚¹": "-", "æ¥æº": "System", "é“¾æ¥": "#"}]

    df = pd.DataFrame(all_jobs)
    today_str = datetime.now().strftime("%Y-%m-%d")

    # A. ä¿å­˜åˆ° Excel
    file_name = "remote_jobs_list.xlsx"
    try:
        if os.path.exists(file_name):
            with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
                df.to_excel(writer, sheet_name=today_str, index=False)
        else:
            df.to_excel(file_name, sheet_name=today_str, index=False)
    except Exception as e:
        print(f"Excel ä¿å­˜å¤±è´¥: {e}")

    # B. æ›´æ–° README
    if os.path.exists("README.md"):
        with open("README.md", "r", encoding="utf-8") as f:
            content = f.read()
        
        # å°†èŒä½è½¬ä¸º Markdown è¡¨æ ¼
        md_table = df.to_markdown(index=False)
        start_tag, end_tag = "", ""
        
        if start_tag in content and end_tag in content:
            new_block = f"{start_tag}\n\n### ğŸ“… æ›´æ–°æ—¥æœŸ: {today_str}\n\n{md_table}\n\n{end_tag}"
            # ä½¿ç”¨æ­£åˆ™æ›¿æ¢ï¼Œç¡®ä¿åªæ›¿æ¢ä¸€å¯¹æ ‡ç­¾ä¹‹é—´çš„å†…å®¹
            pattern = f"{re.escape(start_tag)}.*?{re.escape(end_tag)}"
            updated_content = re.sub(pattern, new_block, content, flags=re.DOTALL)
            
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(updated_content)
            print("âœ… README.md æ›´æ–°æˆåŠŸï¼")
        else:
            print("âŒ é”™è¯¯ï¼šREADME.md ä¸­æ‰¾ä¸åˆ°æš—å·æ ‡ç­¾ï¼")

# --- 4. è¿è¡Œå…¥å£ (ä¸¥æ ¼å¯¹åº”ä¸Šé¢çš„å‡½æ•°å) ---

if __name__ == "__main__":
    print(f"--- ä»»åŠ¡å¯åŠ¨: {datetime.now()} ---")
    combined_data = []
    
    # åªè°ƒç”¨ä¸Šé¢å®šä¹‰è¿‡çš„å‡½æ•°
    combined_data += scrape_wwr_rss()
    time.sleep(2)
    combined_data += scrape_upwork_rss()
    
    # æ‰§è¡Œä¿å­˜
    save_and_update(combined_data)
    print("--- ä»»åŠ¡ç»“æŸ ---")
